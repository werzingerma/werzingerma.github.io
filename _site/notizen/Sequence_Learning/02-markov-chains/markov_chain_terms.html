<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Wichtige Begriffe &amp; Konzepte | Mein Portfolio &amp; Wiki</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Wichtige Begriffe &amp; Konzepte" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Mein persönliches Portfolio, meine Projekte und Notizen" />
<meta property="og:description" content="Mein persönliches Portfolio, meine Projekte und Notizen" />
<link rel="canonical" href="http://localhost:4000/notizen/Sequence_Learning/02-markov-chains/markov_chain_terms" />
<meta property="og:url" content="http://localhost:4000/notizen/Sequence_Learning/02-markov-chains/markov_chain_terms" />
<meta property="og:site_name" content="Mein Portfolio &amp; Wiki" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Wichtige Begriffe &amp; Konzepte" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Mein persönliches Portfolio, meine Projekte und Notizen","headline":"Wichtige Begriffe &amp; Konzepte","url":"http://localhost:4000/notizen/Sequence_Learning/02-markov-chains/markov_chain_terms"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Mein Portfolio &amp; Wiki" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Mein Portfolio &amp; Wiki</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/projekte/">Meine Projekte</a><a class="page-link" href="/notizen/">Meine Notizen</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Wichtige Begriffe &amp; Konzepte</h1>
  </header>

  <div class="post-content">
    <h2 id="markovannahme">Markov‑Annahme</h2>

<p>Der Folgezustand eines Prozesses hängt <strong>ausschließlich</strong> vom aktuellen Zustand ab, nicht von weiter zurückliegender Vergangenheit. Das erlaubt eine kompakte Beschreibung als Übergangs­matrix bzw. gerichteten Graphen.</p>

<h3 id="beispiel--code">Beispiel / Code</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Wir referenzieren das Wetter‑Beispiel aus markov_chain_example1.md
</span><span class="n">next_tag</span> <span class="o">=</span> <span class="n">next_state</span><span class="p">(</span><span class="s">'sunny'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">next_tag</span><span class="p">)</span>  <span class="c1"># → z. B. 'cloudy' oder 'sunny'
</span></code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">next_state</code> berücksichtigt nur den aktuellen Zustand (<code class="language-plaintext highlighter-rouge">'sunny'</code>) – die gesamte Sequenz davor spielt keine Rolle. So manifestiert sich die Markov‑Annahme praktisch im Code.</p>

<h3 id="markov-annahme-vs-markov-kette">Markov Annahme vs Markov-Kette</h3>

<ul>
  <li>Die Markov-Annahme ist die Aussage, dass der nächste Zustand nur vom aktuellen Zustand abhängt.</li>
  <li>Eine Markov-Kette ist ein konkretes stochastisches Modell / Prozess, der genau diese Annahme erfüllt.</li>
</ul>

<hr />

<h2 id="tokenvstype">Token vs. Type</h2>

<ul>
  <li><strong>Token</strong> → jedes einzelne Auftreten eines Wortes in einem Text.</li>
  <li><strong>Type</strong>  → die eindeutige Wortform (»Wörterbuch­eintrag«).
Ein Text mit 100 Tokens kann z. B. 60 Types enthalten, wenn manche Wörter mehrfach vorkommen.</li>
</ul>

<h3 id="beispiel--code-1">Beispiel / Code</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">text</span> <span class="o">=</span> <span class="s">"Make America great again, make America smart again."</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="n">lower</span><span class="p">().</span><span class="n">split</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>           <span class="c1"># 8 Tokens
</span><span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">tokens</span><span class="p">)))</span>      <span class="c1"># 6 Types
</span></code></pre></div></div>

<p>Obwohl 8 Wörter (Tokens) vorkommen, sind es nur 6 verschiedene Wortformen (Types) – »make« und »america« erscheinen jeweils doppelt. Diese Unterscheidung ist wichtig, wenn man Häufigkeiten und Wahrscheinlichkeiten schätzt.</p>

<hr />

<h2 id="wahrscheinlichkeitsabschätzung">Wahrscheinlichkeits­abschätzung</h2>

<p>Die einfachste Schätzung der Auftritts­wahrscheinlichkeit eines Ereignisses ist die <strong>relative Häufigkeit</strong>:
$\hat P(x) = \frac{C(x)}{N}$</p>

<p><em>$C(x)$: Anzahl der Beobachtungen</em><br />
<em>$N$: Gesamtzahl aller Beobachtungen</em></p>

<h3 id="beispiel--code-2">Beispiel / Code</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="n">words</span> <span class="o">=</span> <span class="s">"a a a b b c"</span><span class="p">.</span><span class="n">split</span><span class="p">()</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
<span class="n">P_a</span> <span class="o">=</span> <span class="n">C</span><span class="p">[</span><span class="s">'a'</span><span class="p">]</span> <span class="o">/</span> <span class="n">N</span>
<span class="k">print</span><span class="p">(</span><span class="n">P_a</span><span class="p">)</span>   <span class="c1"># 0.5
</span></code></pre></div></div>

<p>Von 6 Tokens sind 3 mal <code class="language-plaintext highlighter-rouge">'a'</code> aufgetreten → $\hat P(a)=3/6=0{,}5$. Dieses Prinzip verwenden wir auch beim Zählen von N‑Grams (z. B. Bi‑gram‑Wahrscheinlichkeiten).</p>

<hr />

<h2 id="zipfverteilung">Zipf‑Verteilung</h2>

<p>In vielen natürlichsprachlichen Korpora folgt die Rang‑Häufigkeits‑Kurve etwa dem Gesetz:
<em>Häufigkeit ≈ 1 / Rang</em>
→ wenige Wörter sind extrem häufig, sehr viele Wörter extrem selten.</p>

<p><img src="/assets/images/zipf_verteilung.png" alt="Zipf‑Verteilung" /></p>

<h3 id="beispiel--code-3">Beispiel / Code</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">import</span> <span class="nn">re</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'alice.txt'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">findall</span><span class="p">(</span><span class="sa">r</span><span class="s">"\w+"</span><span class="p">,</span> <span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">().</span><span class="n">lower</span><span class="p">())</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

<span class="n">ranks</span>  <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="n">frequ</span>  <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">counts</span><span class="p">.</span><span class="n">most_common</span><span class="p">()]</span>

<span class="n">plt</span><span class="p">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">ranks</span><span class="p">,</span> <span class="n">frequ</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Zipf‑Plot – Alice im Wunderland"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Rang"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Häufigkeit"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>In der doppelt logarithmischen Darstellung bildet die Kurve nahezu eine Gerade – das Charakteristikum der Zipf‑Verteilung. Dies erklärt, warum bei N‑Gram‑Modellen viele mögliche Wort­folgen nie beobachtet werden; <strong>Smoothing</strong> (z. B. Laplace) wird nötig, um Null‑Wahrscheinlichkeiten zu vermeiden.</p>

<h3 id="smoothing">Smoothing</h3>
<p><strong>Smoothing</strong> (Glättung) gibt auch nie beobachteten N-Grammen eine kleine, von den beobachteten Häufigkeiten abgezweigte Wahrscheinlichkeit, indem man z. B. bei der Laplace-Methode zu jedem Zähler 1 addiert.
Hat man nur die Bigramme „die Katze“ (3×) und „die Maus“ (1×) gesehen, aber nie „die Kuh“, erhält man mit Laplace $P(\text{‚Kuh‘}|\text{‚die‘}) = (0+1)/(3+1+0+3) = 1/7$ statt 0.</p>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Mein Portfolio &amp; Wiki</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Mein Portfolio &amp; Wiki</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/werzingerma"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">werzingerma</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Mein persönliches Portfolio, meine Projekte und Notizen</p>
      </div>
    </div>

  </div>

</footer>
<!-- MathJax für LaTeX-Formeln -->
    <script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    },
    options: {
      skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      processHtmlClass: 'tex2jax_process'
    }
  };
</script>
<script type="text/javascript" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
  </body>

</html>
